# -*- coding: utf-8 -*-
"""Distributed Deep Learning with Horovod

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GOvKuSXDBvOEvQyG4Y51SY_y7pDHiwQg
"""

!pip install horovod[tensorflow,keras,pytorch,mxnet,spark]

from google.colab import drive
drive.mount('/content/gdrive')

import tensorflow as tf
from numpy import mean,std
from sklearn.model_selection import KFold
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten
from tensorflow.keras.optimizers import SGD
from matplotlib import pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras import models
import horovod.tensorflow.keras as hvd
import numpy as np
import argparse
import time
import sys

def load_dataset():
    (trainX, trainY), (testX, testY) = mnist.load_data()
    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
    testX = testX.reshape((testX.shape[0], 28, 28, 1))
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
  
  def prep_pixels(train, test):
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    return train_norm, test_norm
  
  def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(10, activation='softmax'))
    opt = tf.keras.optimizers.SGD(0.0005 * hvd.size())
    opt = hvd.DistributedOptimizer(opt)
    model.compile(loss='categorical_crossentropy',optimizer=opt,
                metrics=['accuracy'],experimental_run_tf_function=False)
    return model
  
  def evaluate_model(dataX, dataY, n_folds=5):
    
    scores, histories = list(), list()
    kfold = KFold(n_folds, shuffle=True, random_state=1)
    
    hvd.init()
    
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    if gpus:
      tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')
    
    callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0)]

    for train_ix, test_ix in kfold.split(dataX):
      model = define_model()
      trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]
      history = model.fit(trainX, trainY, epochs=epochs,validation_data=(testX, testY), callbacks=callbacks,verbose=1 if hvd.rank() == 0 else 0)
      _, acc = model.evaluate(testX, testY, verbose=0)
      print('> %.3f' % (acc * 100.0))
      scores.append(acc)
      histories.append(history)
    
    return scores, histories
  
  def summarize_diagnostics(histories):
    for i in range(len(histories)):
      plt.subplot(2, 1, 1)
      plt.title('Cross Entropy Loss')
      plt.plot(histories[i].history['loss'], color='blue', label='train')
      plt.plot(histories[i].history['val_loss'], color='orange', label='test')
      plt.subplot(2, 1, 2)
      plt.title('Classification Accuracy')
      plt.plot(histories[i].history['accuracy'], color='blue', label='train')
      plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')
    plt.show()
  
  def summarize_performance(scores):
    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))
    plt.boxplot(scores)
    plt.show()
  
  def run_test_harness():
    trainX, trainY, testX, testY = load_dataset()
    trainX, testX = prep_pixels(trainX, testX)
    scores, histories = evaluate_model(trainX, trainY)
    summarize_diagnostics(histories)
    summarize_performance(scores)
  
  run_test_harness()